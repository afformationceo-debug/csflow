import OpenAI from "openai";
import Anthropic from "@anthropic-ai/sdk";
import { Tenant } from "@/lib/supabase/types";
import { RetrievedDocument } from "./retriever";

// Lazy initialization of clients
let openai: OpenAI | null = null;
let anthropic: Anthropic | null = null;

function getOpenAIClient(): OpenAI {
  if (!openai) {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error("OPENAI_API_KEY environment variable is not set");
    }
    openai = new OpenAI({ apiKey });
  }
  return openai;
}

function getAnthropicClient(): Anthropic {
  if (!anthropic) {
    const apiKey = process.env.ANTHROPIC_API_KEY;
    if (!apiKey) {
      throw new Error("ANTHROPIC_API_KEY environment variable is not set");
    }
    anthropic = new Anthropic({ apiKey });
  }
  return anthropic;
}

export type LLMModel = "gpt-4" | "gpt-4-turbo" | "claude-3-opus" | "claude-3-sonnet";

export interface LLMResponse {
  content: string;
  model: LLMModel;
  confidence: number;
  tokensUsed: number;
  processingTimeMs: number;
}

export interface GenerateOptions {
  model?: LLMModel;
  temperature?: number;
  maxTokens?: number;
  tenantConfig?: Tenant["ai_config"];
}

/**
 * Build context from retrieved documents
 */
function buildContext(documents: RetrievedDocument[]): string {
  if (documents.length === 0) {
    return "ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.";
  }

  return documents
    .map(
      (doc, i) =>
        `[ë¬¸ì„œ ${i + 1}] ${doc.title}\n${doc.chunkText}\n(ìœ ì‚¬ë„: ${(
          doc.similarity * 100
        ).toFixed(1)}%)`
    )
    .join("\n\n");
}

/**
 * Build system prompt for tenant
 */
function buildSystemPrompt(
  tenantConfig: Tenant["ai_config"],
  context: string
): string {
  const config = tenantConfig as {
    system_prompt?: string;
    hospital_name?: string;
    specialty?: string;
  };

  const basePrompt = config?.system_prompt || getDefaultSystemPrompt();

  return `${basePrompt}

## ë³‘ì› ì •ë³´
- ë³‘ì›ëª…: ${config?.hospital_name || "ì •ë³´ ì—†ìŒ"}
- ì „ë¬¸ ë¶„ì•¼: ${config?.specialty || "ì •ë³´ ì—†ìŒ"}

## ì°¸ê³  ìë£Œ
${context}

## ì‘ë‹µ ê°€ì´ë“œë¼ì¸
1. **ì¤‘ìš”**: í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”. ê³ ê°ì´ ì–´ë–¤ ì–¸ì–´ë¡œ ì§ˆë¬¸í•˜ë“  ìƒê´€ì—†ì´ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë°˜ë“œì‹œ ì°¸ê³  ìë£Œì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
3. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” "ë‹´ë‹¹ìì—ê²Œ í™•ì¸ í›„ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”.
4. ì˜ë£Œì  ì¡°ì–¸ì€ ì§ì ‘ ì œê³µí•˜ì§€ ë§ê³ , ìƒë‹´ ì˜ˆì•½ì„ ê¶Œìœ í•˜ì„¸ìš”.
5. ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.
6. ê°€ê²© ì •ë³´ëŠ” ì •í™•í•œ ê²½ìš°ì—ë§Œ ì•ˆë‚´í•˜ì„¸ìš”.`;
}

function getDefaultSystemPrompt(): string {
  return `# ì—­í•  ì •ì˜
ë‹¹ì‹ ì€ ìµœê³ ê¸‰ ì˜ë£Œ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ ì •ë³´ ì „ë‹¬ìê°€ ì•„ë‹Œ, ê³ ê°ì˜ ê³ ë¯¼ì„ í•´ê²°í•˜ê³  ìµœì ì˜ ì˜ë£Œ ì„œë¹„ìŠ¤ë¡œ ì´ë„ëŠ” ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

# í•µì‹¬ ë¯¸ì…˜ (ìˆœì°¨ì ìœ¼ë¡œ ë‹¬ì„±)

## 1ë‹¨ê³„: ë¼í¬ í˜•ì„± & ë‹ˆì¦ˆ íŒŒì•…
- ê³ ê°ì˜ ê°ì •ì„ ì½ê³  ê³µê°í•©ë‹ˆë‹¤ (ë¶ˆì•ˆ, ê¸°ëŒ€, ë§ì„¤ì„ ë“±)
- ê°œë°©í˜• ì§ˆë¬¸ìœ¼ë¡œ ì§„ì§œ ë‹ˆì¦ˆë¥¼ ë°œêµ´í•©ë‹ˆë‹¤
  ì˜ˆ: "ì–´ë–¤ ë¶€ë¶„ì´ ê°€ì¥ ê±±ì •ë˜ì‹œë‚˜ìš”?", "ì´ë²ˆ ì‹œìˆ ì„ ê³ ë ¤í•˜ê²Œ ëœ íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆìœ¼ì‹ ê°€ìš”?"
- ê³ ê°ì˜ ìƒí™©(ì˜ˆì‚°, íšŒë³µ ê¸°ê°„, ë¼ì´í”„ìŠ¤íƒ€ì¼)ì„ ìì—°ìŠ¤ëŸ½ê²Œ íŒŒì•…í•©ë‹ˆë‹¤

## 2ë‹¨ê³„: ë§ì¶¤ ì†”ë£¨ì…˜ ì œì‹œ
- ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê°ì—ê²Œ ìµœì ì¸ ì˜µì…˜ì„ ì œì•ˆí•©ë‹ˆë‹¤
- ì¥ì ë¿ë§Œ ì•„ë‹ˆë¼ ì£¼ì˜ì‚¬í•­ë„ íˆ¬ëª…í•˜ê²Œ ì•ˆë‚´í•©ë‹ˆë‹¤ (ì‹ ë¢° êµ¬ì¶•)
- ê°€ê²©ì´ ë¶€ë‹´ìŠ¤ëŸ¬ìš¸ ê²½ìš° í• ë¶€ë‚˜ ëŒ€ì•ˆ ì‹œìˆ ì„ ì œì•ˆí•©ë‹ˆë‹¤

## 3ë‹¨ê³„: ë‹¤ìŒ ì•¡ì…˜ ìœ ë„ (ìì—°ìŠ¤ëŸ¬ìš´ CTA)
- ìƒë‹´ ì˜ˆì•½ ê¶Œìœ : "ë” ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ ë¬´ë£Œ ìƒë‹´ì„ ì˜ˆì•½í•´ë“œë¦´ê¹Œìš”?"
- ì¶”ê°€ ì •ë³´ ì œê³µ: "ê´€ë ¨ Before/After ì‚¬ì§„ì„ ë³´ë‚´ë“œë¦´ê¹Œìš”?"
- ì¦‰ì‹œ ê²°ì • ìœ ë„ (ë‹¨, ì••ë°• ê¸ˆì§€): "ì´ë²ˆ ì£¼ ì˜ˆì•½ ì‹œ í• ì¸ í˜œíƒì´ ìˆìŠµë‹ˆë‹¤"

## 4ë‹¨ê³„: ì˜ˆì•½ í›„ ì‚¬í›„ ê´€ë¦¬
- ì˜ˆì•½ í™•ì • ì‹œ: ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­, ë°©ë¬¸ ì•ˆë‚´, ì£¼ì˜ì‚¬í•­ ì „ë‹¬
- ì‹œìˆ  í›„: íšŒë³µ ê²½ê³¼ í™•ì¸, ë¶ˆí¸ ì‚¬í•­ ì²´í¬, ì¬ë°©ë¬¸ ìœ ë„
- ë§Œì¡±ë„ ì¡°ì‚¬: "ì‹œìˆ  ê²°ê³¼ì— ë§Œì¡±í•˜ì…¨ë‚˜ìš”? ë¦¬ë·°ë¥¼ ë‚¨ê²¨ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤"

# ìƒë‹´ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ

## í†¤ ì•¤ ë§¤ë„ˆ
âœ… ë”°ëœ»í•˜ê³  ì¹œê·¼í•˜ì§€ë§Œ ì „ë¬¸ì„± ìˆëŠ” í†¤
âœ… ê³ ê°ì˜ ì–¸ì–´ë¡œ ì„¤ëª… (ì „ë¬¸ ìš©ì–´ ìµœì†Œí™” â†’ ì‰½ê²Œ í’€ì–´ì“°ê¸°)
âœ… ì´ëª¨ì§€ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ 1-2ê°œë§Œ (ê³¼ë„í•œ ì‚¬ìš© ê¸ˆì§€)
âœ… ê³ ê°ì´ VIPì²˜ëŸ¼ ëŠë¼ê²Œ í•˜ëŠ” ì„¸ì‹¬í•œ ë°°ë ¤

## ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­
âŒ "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" â†’ ëŒ€ì‹  "ë‹´ë‹¹ ì „ë¬¸ì˜ì—ê²Œ ì§ì ‘ í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤"
âŒ ì˜ë£Œ ì§„ë‹¨ ë˜ëŠ” ì²˜ë°© (ë²•ì  ë¬¸ì œ)
âŒ ê²½ìŸ ë³‘ì› ë¹„ë‚œ ë˜ëŠ” ë¹„êµ
âŒ ê³¼ë„í•œ ì••ë°• ì˜ì—… (ê³ ê° ë°˜ê° ìœ ë°œ)
âŒ ê°œì¸ì •ë³´ ë…¸ì¶œ (HIPAA/GDPR ìœ„ë°˜)

## ëŒ€í™” íë¦„ ì˜ˆì‹œ (Best Practice)

### ì´ˆê¸° ë¬¸ì˜ ì‘ëŒ€
ê³ ê°: "ë¼ì‹ ìˆ˜ìˆ  ë¹„ìš©ì´ ì–¼ë§ˆì¸ê°€ìš”?"
ìƒë‹´ì‚¬: "ì•ˆë…•í•˜ì„¸ìš”! ë¼ì‹ ìˆ˜ìˆ ì— ê´€ì‹¬ ê°–ê³  ê³„ì‹œëŠ”êµ°ìš” ğŸ˜Š ì €í¬ ë³‘ì›ì€ ì–‘ì•ˆ ê¸°ì¤€ 150ë§Œì›ì´ë©°, ë¬´ì´ì í• ë¶€ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤. í˜¹ì‹œ ë¼ì‹ì„ ê³ ë ¤í•˜ì‹œê²Œ ëœ íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆìœ¼ì‹ ê°€ìš”? (ì˜ˆ: ì•ˆê²½ ë¶ˆí¸, ìš´ë™, ì§ì—… ë“±)"

### ë¶ˆì•ˆ í•´ì†Œ
ê³ ê°: "ë¶€ì‘ìš©ì´ ê±±ì •ë¼ìš”..."
ìƒë‹´ì‚¬: "ì¶©ë¶„íˆ ê±±ì •ë˜ì‹¤ ìˆ˜ ìˆì–´ìš”. ë¼ì‹ ìˆ˜ìˆ ì€ FDA ìŠ¹ì¸ì„ ë°›ì€ ì•ˆì „í•œ ì‹œìˆ ì´ì§€ë§Œ, ëª¨ë“  ìˆ˜ìˆ ì´ ê·¸ë ‡ë“¯ ê°œì¸ì°¨ëŠ” ìˆìŠµë‹ˆë‹¤. ì €í¬ëŠ” ìˆ˜ìˆ  ì „ ì •ë°€ ê²€ì‚¬ë¡œ ì í•©ì„±ì„ ì² ì €íˆ íŒë‹¨í•˜ë©°, 1ë…„ê°„ ë¬´ë£Œ ì‚¬í›„ ê´€ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ì œ ìˆ˜ìˆ  ê²½í—˜ë‹´ì„ ë“¤ì–´ë³´ì‹œëŠ” ê±´ ì–´ë– ì„¸ìš”?"

### ì˜ˆì•½ ì „í™˜
ê³ ê°: "ì¢€ ë” ìƒê°í•´ë³¼ê²Œìš”."
ìƒë‹´ì‚¬: "ë„¤, ì¶©ë¶„íˆ ê³ ë¯¼í•˜ì‹œëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤! ë‹¤ë§Œ, ì •í™•í•œ ìƒë‹´ì€ ì§ì ‘ ëˆˆ ìƒíƒœë¥¼ í™•ì¸í•´ì•¼ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë¬´ë£Œ ê²€ì‚¬ ì˜ˆì•½ì€ ë¶€ë‹´ ì—†ì´ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìœ¼ë‹ˆ, ì¼ì • ì¡ì•„ë“œë¦´ê¹Œìš”? ğŸ˜Š ê²€ì‚¬ í›„ ìˆ˜ìˆ  ì—¬ë¶€ëŠ” ì²œì²œíˆ ê²°ì •í•˜ì…”ë„ ë©ë‹ˆë‹¤."

### ì˜ˆì•½ í™•ì • í›„
ìƒë‹´ì‚¬: "ì˜ˆì•½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰ ë°©ë¬¸ ì „ ë Œì¦ˆëŠ” 3ì¼ ì „ë¶€í„° ì°©ìš©í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”. ë‹¹ì¼ ì¤€ë¹„ë¬¼ê³¼ ì£¼ì°¨ ì •ë³´ëŠ” ë¬¸ìë¡œ ë°œì†¡í•´ë“œë ¸ìŠµë‹ˆë‹¤. ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì—°ë½ì£¼ì„¸ìš”!"

# ìƒí™©ë³„ ëŒ€ì‘ ì „ëµ

## ê°€ê²© í¥ì • ì‹œ
"ê³ ê°ë‹˜ì˜ ì˜ˆì‚°ì„ ì´í•´í•©ë‹ˆë‹¤. í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ë“œë¦´ê²Œìš”. ë˜í•œ 3ê°œì›” ë¬´ì´ì í• ë¶€ë„ ê°€ëŠ¥í•˜ë‹ˆ ë¶€ë‹´ì„ ì¤„ì´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

## ê²½ìŸì‚¬ ë¹„êµ ì‹œ
"ë‹¤ë¥¸ ë³‘ì›ê³¼ ë¹„êµí•˜ì‹œëŠ” ê²ƒë„ í˜„ëª…í•œ ì„ íƒì…ë‹ˆë‹¤. ì €í¬ëŠ” [ì°¨ë³„í™” í¬ì¸íŠ¸: ì •í’ˆ ë³´ì¦, ê²½ë ¥ 20ë…„ ì „ë¬¸ì˜, ì‚¬í›„ ê´€ë¦¬ ë“±]ì— ìì‹  ìˆìŠµë‹ˆë‹¤. ì§ì ‘ ë¹„êµ ìƒë‹´ ë°›ì•„ë³´ì‹œëŠ” ê±´ ì–´ë– ì„¸ìš”?"

## ë¶€ì •ì  ë¦¬ë·° ì–¸ê¸‰ ì‹œ
"ê±±ì •ë˜ì‹œëŠ” ë¶€ë¶„ ì¶©ë¶„íˆ ì´í•´í•©ë‹ˆë‹¤. ëª¨ë“  ì‚¬ë¡€ë¥¼ íˆ¬ëª…í•˜ê²Œ ê³µê°œí•˜ë©°, ë§Œì•½ ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ì±…ì„ì§€ê³  ì¬ì‹œìˆ ì„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ì œ ê³ ê° í›„ê¸°ë¥¼ ë³´ì—¬ë“œë¦´ê¹Œìš”?"

## ê¸´ê¸‰ ìƒí™© (í†µì¦, ì¶œí˜ˆ ë“±)
"ì¦‰ì‹œ ë³‘ì›ìœ¼ë¡œ ì—°ë½í•˜ì‹œê±°ë‚˜ ì‘ê¸‰ì‹¤ ë°©ë¬¸ì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤. ì—°ë½ì²˜: [ë³‘ì› ì „í™”ë²ˆí˜¸]. ì œê°€ ì§€ê¸ˆ ë°”ë¡œ ë‹´ë‹¹ìë¥¼ ì—°ê²°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!"`;
}

/**
 * Calculate confidence score based on retrieved documents and response
 */
function calculateConfidence(
  documents: RetrievedDocument[],
  response: string,
  query: string
): number {
  // Base confidence from document relevance
  const avgSimilarity =
    documents.length > 0
      ? documents.reduce((sum, d) => sum + d.similarity, 0) / documents.length
      : 0;

  // Penalty for uncertainty phrases
  const uncertaintyPhrases = [
    "í™•ì‹¤í•˜ì§€ ì•Š",
    "ì •í™•íˆ ëª¨ë¥´",
    "ë‹´ë‹¹ìì—ê²Œ í™•ì¸",
    "í™•ì¸ì´ í•„ìš”",
    "ì˜ ëª¨ë¥´ê² ",
  ];
  const hasUncertainty = uncertaintyPhrases.some((phrase) =>
    response.includes(phrase)
  );

  // Penalty for no context
  const noContextPenalty = documents.length === 0 ? 0.3 : 0;

  // Calculate final confidence
  let confidence = avgSimilarity * 0.6 + 0.4; // Base 40% + up to 60% from similarity
  if (hasUncertainty) confidence -= 0.15;
  confidence -= noContextPenalty;

  return Math.max(0, Math.min(1, confidence));
}

/**
 * Generate response using GPT-4
 */
async function generateWithGPT(
  query: string,
  context: string,
  tenantConfig: Tenant["ai_config"],
  options: GenerateOptions
): Promise<Omit<LLMResponse, "confidence">> {
  const startTime = Date.now();

  const systemPrompt = buildSystemPrompt(tenantConfig, context);
  const model = options.model === "gpt-4-turbo" ? "gpt-4-turbo" : "gpt-4";

  const client = getOpenAIClient();
  const response = await client.chat.completions.create({
    model,
    messages: [
      { role: "system", content: systemPrompt },
      { role: "user", content: query },
    ],
    temperature: options.temperature ?? 0.7,
    max_tokens: options.maxTokens ?? 1000,
  });

  return {
    content: response.choices[0].message.content || "",
    model: options.model || "gpt-4",
    tokensUsed: response.usage?.total_tokens || 0,
    processingTimeMs: Date.now() - startTime,
  };
}

/**
 * Generate response using Claude
 */
async function generateWithClaude(
  query: string,
  context: string,
  tenantConfig: Tenant["ai_config"],
  options: GenerateOptions
): Promise<Omit<LLMResponse, "confidence">> {
  const startTime = Date.now();

  const systemPrompt = buildSystemPrompt(tenantConfig, context);
  const model =
    options.model === "claude-3-opus"
      ? "claude-3-opus-20240229"
      : "claude-3-sonnet-20240229";

  const client = getAnthropicClient();
  const response = await client.messages.create({
    model,
    max_tokens: options.maxTokens ?? 1000,
    system: systemPrompt,
    messages: [{ role: "user", content: query }],
  });

  const content =
    response.content[0].type === "text" ? response.content[0].text : "";

  return {
    content,
    model: options.model || "claude-3-sonnet",
    tokensUsed: response.usage.input_tokens + response.usage.output_tokens,
    processingTimeMs: Date.now() - startTime,
  };
}

/**
 * Main LLM service
 */
export const llmService = {
  /**
   * Generate response with RAG context
   */
  async generate(
    query: string,
    documents: RetrievedDocument[],
    options: GenerateOptions = {}
  ): Promise<LLMResponse> {
    const context = buildContext(documents);
    const model = options.model || "gpt-4";

    let response: Omit<LLMResponse, "confidence">;

    if (model.startsWith("claude")) {
      response = await generateWithClaude(
        query,
        context,
        options.tenantConfig || {},
        options
      );
    } else {
      response = await generateWithGPT(
        query,
        context,
        options.tenantConfig || {},
        options
      );
    }

    const confidence = calculateConfidence(documents, response.content, query);

    return {
      ...response,
      confidence,
    };
  },

  /**
   * Simple query without RAG context (for classification, etc.)
   */
  async query(params: {
    systemPrompt: string;
    userPrompt: string;
    model?: "gpt-4" | "claude-3-sonnet";
    temperature?: number;
    maxTokens?: number;
  }): Promise<{ text: string }> {
    const model = params.model || "gpt-4";
    const temperature = params.temperature ?? 0.7;
    const maxTokens = params.maxTokens || 1000;

    if (model.startsWith("claude")) {
      const anthropic = getAnthropicClient();
      const response = await anthropic.messages.create({
        model: "claude-3-sonnet-20240229",
        max_tokens: maxTokens,
        temperature,
        system: params.systemPrompt,
        messages: [{ role: "user", content: params.userPrompt }],
      });
      return { text: response.content[0].type === 'text' ? response.content[0].text : '' };
    } else {
      const openai = getOpenAIClient();
      const response = await openai.chat.completions.create({
        model: "gpt-4",
        messages: [
          { role: "system", content: params.systemPrompt },
          { role: "user", content: params.userPrompt },
        ],
        temperature,
        max_tokens: maxTokens,
      });
      return { text: response.choices[0].message.content || "" };
    }
  },

  /**
   * Select best model based on query complexity
   */
  selectModel(query: string, documentCount: number): LLMModel {
    const isComplex =
      query.length > 200 ||
      documentCount > 5 ||
      /ì˜ë£Œ|ìˆ˜ìˆ |í•©ë³‘ì¦|ë¶€ì‘ìš©/.test(query);

    // Use Claude for complex medical queries
    if (isComplex) {
      return "claude-3-sonnet";
    }

    // Use GPT-4 for general queries
    return "gpt-4";
  },

  /**
   * Generate AI suggestion for agent
   */
  async generateSuggestion(
    query: string,
    documents: RetrievedDocument[],
    conversationHistory: { role: "user" | "assistant"; content: string }[]
  ): Promise<string> {
    const context = buildContext(documents);

    const historyText = conversationHistory
      .slice(-6) // Last 3 exchanges
      .map((m) => `${m.role === "user" ? "ê³ ê°" : "ìƒë‹´ì‚¬"}: ${m.content}`)
      .join("\n");

    const prompt = `ë‹¤ìŒ ëŒ€í™” ë§¥ë½ê³¼ ê³ ê°ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ë³´ê³ , ìƒë‹´ì‚¬ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

## ëŒ€í™” ê¸°ë¡
${historyText}

## ê³ ê° ë©”ì‹œì§€
${query}

## ì°¸ê³  ìë£Œ
${context}

ìƒë‹´ì‚¬ê°€ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:`;

    const client = getOpenAIClient();
    const response = await client.chat.completions.create({
      model: "gpt-4-turbo",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.7,
      max_tokens: 500,
    });

    return response.choices[0].message.content || "";
  },
};
